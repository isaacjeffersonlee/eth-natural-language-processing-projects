{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84,
     "referenced_widgets": [
      "b53f65341be24333939cd373696bd403",
      "de8774404f7c4879950013b96feadc0a",
      "9969e49d1734463aa3a7fc133a41f8c1",
      "ca22e587adde43f09d124961d24e9586",
      "c4e2088b7959485fb58b4db74d924ab2",
      "373974851b6b40b58c157fbd4ee63eae",
      "74d55b0c6da54a8dbd5391ce1decbd3c",
      "2f91928b30444f48ba9b3996b637c651",
      "28ca8be1f7564e1ba9139de0b4faae88",
      "ea3b17a58fac4d9f974d9200f29983f6",
      "d540aa212f6a4335a375b242cbafe235"
     ]
    },
    "id": "5fQf34J3cJY-",
    "outputId": "03ad9061-cb70-4dc6-8a91-6bfadeded01d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/isaac/.virtualenvs/nlp/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import libraries used throughout - if you need other libraries, \n",
    "# you are free to import them.\n",
    "import functools\n",
    "import random\n",
    "from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchtext\n",
    "import torchtext.functional as F\n",
    "import torchtext.transforms\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.datasets import UDPOS\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from transformers import BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "F4OW-ryOCwbG"
   },
   "outputs": [],
   "source": [
    "# Constants and hyperparameters - you are free to change these for\n",
    "# the bonus question.\n",
    "SEED = 42\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 3\n",
    "LR = 2e-5\n",
    "TRANSFORMER = \"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "lYinc-SdCzdZ"
   },
   "outputs": [],
   "source": [
    "# Reproducibility.\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "a80ce5e04f6f4bd4a34f551c44574503",
      "c5cf1737a8e641fba46c4a01c69d1305",
      "3b5662f987654b3e97d41a8a88f3dec5",
      "620bfa45d32a4f54975c80619759be15",
      "c94dbb98558a4dd296eb8de93d425176",
      "5b71b3392fb04c3a864051627fe20d22",
      "4b268bd0b12a4249a9dd0ff908538c7f",
      "18bbe330643f4b198691ef13619fb7af",
      "a6dda7b753b040d693cc141d3cd2f150",
      "c55a934128244302851e9f140786f497",
      "d2ed12c99aa74a37b11c5febfa35097f",
      "43d6ebaa410045d0bf729345ea25172a",
      "5b6f142f2844455cab53a7fda33f4cb6",
      "cdeaffb2377f49bdbd0ef9c459542a5f",
      "0083e082a6c7445dbdad6dc53066bd05",
      "4b5b236ab1e34425826c6396b71257a3",
      "1f9d911439044e46b9f89d0aea4ce41e",
      "a6606733bcdb4f6ea7f6d4a90ace57b7",
      "631673d7c05b4cbd9c39e77558ade025",
      "a95fbd63e826417ebf8af09444e676d0",
      "473bd59f2a77417481ffd230e64106dc",
      "fe52158d139c44bb9640d92b0cdd663d",
      "505cbe89e2384292aa5c64ecc080d610",
      "7d913c7b1f1b4986a7396bd0a692a0f0",
      "2c338c4a39fe41cb9765c549d84b85b9",
      "533d31dc5c604d9287ad695c6cb96edd",
      "b94d16b7795e4e7485ea8fd564d9bc70",
      "eeda478a90e249978358336ca738c55a",
      "aac730ab374f4f119bdb124fc604d9f0",
      "016cce9cee9343dab3d7253193094520",
      "ae4f9ae44cc64926b06864de8669d468",
      "f9cc3df591bc4122beeed2a3371cfa9f",
      "5729f1f235c44fdf9c04210a1a7cf6c6"
     ]
    },
    "id": "YrTZG0UHC1YU",
    "outputId": "e3c720d4-5292-403e-ba47-14d36715633d"
   },
   "outputs": [],
   "source": [
    "# Setting up dataloaders for training.\n",
    "tokenizer = BertTokenizer.from_pretrained(TRANSFORMER)\n",
    "init_token = tokenizer.cls_token\n",
    "pad_token = tokenizer.pad_token\n",
    "unk_token = tokenizer.unk_token\n",
    "sep_token = tokenizer.sep_token\n",
    "init_token_idx = tokenizer.convert_tokens_to_ids(init_token)\n",
    "pad_token_idx = tokenizer.convert_tokens_to_ids(pad_token)\n",
    "unk_token_idx = tokenizer.convert_tokens_to_ids(unk_token)\n",
    "sep_token_idx = tokenizer.convert_tokens_to_ids(sep_token)\n",
    "max_input_length = tokenizer.max_model_input_sizes[TRANSFORMER]\n",
    "\n",
    "train_datapipe = UDPOS(split=\"train\")\n",
    "valid_datapipe = UDPOS(split=\"valid\")\n",
    "pos_vocab = build_vocab_from_iterator(\n",
    "    [i[1] for i in list(train_datapipe)],\n",
    "    specials=[init_token, pad_token, sep_token],\n",
    ")\n",
    "\n",
    "\n",
    "def prepare_words(tokens, tokenizer, max_input_length, init_token, sep_token):\n",
    "    \"\"\"Preprocesses words such that they may be passed into BERT.\n",
    "\n",
    "    Parameters\n",
    "    ---\n",
    "    tokens : List\n",
    "        List of strings, each of which corresponds to one token in a sequence.\n",
    "    tokenizer : transformers.models.bert.tokenization_bert.BertTokenizer\n",
    "        Tokenizer to be used for transforming word strings into word indices\n",
    "        to be used with BERT.\n",
    "    max_input_length : int\n",
    "        Maximum input length of each sequence as expected by our version of BERT.\n",
    "    init_token : str\n",
    "        String representation of the beginning of sentence marker for our tokenizer.\n",
    "    sep_token : str\n",
    "        String representation of the end of sentence marker for our tokenizer.\n",
    "\n",
    "    Returns\n",
    "    ---\n",
    "    tokens : List\n",
    "        List of preprocessed tokens.\n",
    "    \"\"\"\n",
    "    # Append beginning of sentence and end of sentence markers\n",
    "    # lowercase each token and cut them to the maximum length\n",
    "    # (minus two to account for beginning and end of sentence).\n",
    "    tokens = (\n",
    "        [init_token]\n",
    "        + [i.lower() for i in tokens[: max_input_length - 2]]\n",
    "        + [sep_token]\n",
    "    )\n",
    "    # Convert word strings to indices.\n",
    "    tokens = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def prepare_tags(tokens, max_input_length, init_token, sep_token):\n",
    "    \"\"\"Convert tag strings into indices for use with torch. For symmetry, we perform\n",
    "        identical preprocessing as on our words, even though we do not need beginning\n",
    "        of sentence and end of sentence markers for our tags.\n",
    "\n",
    "    Parameters\n",
    "    ---\n",
    "    tokens : List\n",
    "        List of strings, each of which corresponds to one token in a sequence.\n",
    "    max_input_length : int\n",
    "        Maximum input length of each sequence as expected by our version of BERT.\n",
    "    init_token : str\n",
    "        String representation of the beginning of sentence marker for our tokenizer.\n",
    "    sep_token : str\n",
    "        String representation of the end of sentence marker for our tokenizer.\n",
    "\n",
    "    Returns\n",
    "    ---\n",
    "    tokens : List\n",
    "        List of preprocessed tags.\n",
    "    \"\"\"\n",
    "    # Append beginning of sentence and end of sentence markers\n",
    "    # cut the tagging sequence to the maximum length (minus two to account for beginning and end of sentence).\n",
    "    tokens = [init_token] + tokens[: max_input_length - 2] + [sep_token]\n",
    "    # Convert tag strings to indices.\n",
    "    tokens = torchtext.transforms.VocabTransform(pos_vocab)(tokens)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "text_preprocessor = functools.partial(\n",
    "    prepare_words,\n",
    "    tokenizer=tokenizer,\n",
    "    max_input_length=max_input_length,\n",
    "    init_token=init_token,\n",
    "    sep_token=sep_token,\n",
    ")\n",
    "\n",
    "tag_preprocessor = functools.partial(\n",
    "    prepare_tags,\n",
    "    max_input_length=max_input_length,\n",
    "    init_token=init_token,\n",
    "    sep_token=sep_token,\n",
    ")\n",
    "\n",
    "\n",
    "def apply_transform(x):\n",
    "    return text_preprocessor(x[0]), tag_preprocessor(x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ikF2VKvqC2DS"
   },
   "outputs": [],
   "source": [
    "train_datapipe = (\n",
    "    train_datapipe.map(apply_transform)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .rows2columnar([\"words\", \"pos\"])\n",
    ")\n",
    "train_dataloader = DataLoader(train_datapipe, batch_size=None, shuffle=False)\n",
    "valid_datapipe = (\n",
    "    valid_datapipe.map(apply_transform)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .rows2columnar([\"words\", \"pos\"])\n",
    ")\n",
    "valid_dataloader = DataLoader(valid_datapipe, batch_size=None, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "8AgCIq_AC7K5"
   },
   "outputs": [],
   "source": [
    "class TagLSTM(nn.Module):\n",
    "    \"\"\"Models an LSTM on top of a transformer to predict POS in a Neural CRF.\"\"\"\n",
    "\n",
    "    def __init__(self, nb_labels, emb_dim, hidden_dim=256):\n",
    "        \"\"\"Constructor.\n",
    "\n",
    "        Parameters\n",
    "        ---\n",
    "        nb_labels : int\n",
    "            Number of POS tags to be considered.\n",
    "\n",
    "        emb_dim : int\n",
    "            Input_size of the LSTM - effectively embedding dimension of our pretrained transformer.\n",
    "\n",
    "        hidden_dim : int\n",
    "            Hidden dimension of the LSTM.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(\n",
    "            emb_dim, hidden_dim // 2, bidirectional=True, batch_first=True\n",
    "        )\n",
    "        self.tag = nn.Linear(hidden_dim, nb_labels)\n",
    "        self.hidden = None\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (\n",
    "            torch.randn(2, batch_size, self.hidden_dim // 2),\n",
    "            torch.randn(2, batch_size, self.hidden_dim // 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.hidden = self.init_hidden(x.shape[0])\n",
    "        x, self.hidden = self.lstm(x, self.hidden)\n",
    "        x = self.tag(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified/Simplified from:\n",
    "# https://github.com/rycolab/aflt-f2022/blob/main/rayuela/base/datastructures.py\n",
    "\n",
    "class Vector:\n",
    "\n",
    "    def __init__(self, cap):\n",
    "        self.cap = cap\n",
    "        self.val = np.zeros(self.cap, dtype=np.double)\n",
    "        self.end = 0\n",
    "\n",
    "    def push(self, x):\n",
    "        i = self.end\n",
    "        self.ensure_size(i)\n",
    "        self.val[i] = x\n",
    "        self.end += 1\n",
    "        return i\n",
    "\n",
    "    def pop(self):\n",
    "        \"pop from the end\"\n",
    "        assert 0 < self.end\n",
    "        self.end -= 1\n",
    "        v = self.val[self.end]\n",
    "        self.val[self.end] = np.nan\n",
    "        return v\n",
    "\n",
    "    def grow(self):\n",
    "        self.cap *= 2\n",
    "        new = np.empty(self.cap, dtype=np.double)\n",
    "        new[:self.end] = self.val[:self.end]\n",
    "        self.val = new\n",
    "\n",
    "    def ensure_size(self, i):\n",
    "        \"grow in needed\"\n",
    "        if self.val.shape[0] < i + 1: self.grow()\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        assert i < self.end\n",
    "        return self.get(i)\n",
    "\n",
    "    def get(self, i):\n",
    "        return self.val[i]\n",
    "\n",
    "    def __setitem__(self, i, v):\n",
    "        assert i < self.end\n",
    "        self.set(i, v)\n",
    "\n",
    "    def set(self, i, v):\n",
    "        self.val[i] = v\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.end\n",
    "\n",
    "    def __repr__(self):\n",
    "        return repr(self.val[:self.end])\n",
    "\n",
    "\n",
    "class MaxHeap:\n",
    "\n",
    "    def __init__(self, cap=2**8):\n",
    "        self.val = Vector(cap)\n",
    "        self.val.push(np.nan)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.val) - 1   # subtract one for dummy root element\n",
    "\n",
    "    def pop(self):\n",
    "        v = self.peek()\n",
    "        self._remove(1)\n",
    "        return v\n",
    "\n",
    "    def peek(self):\n",
    "        return self.val.val[1]\n",
    "\n",
    "    def push(self, v):\n",
    "        # put new element last and bubble up\n",
    "        return self.up(self.val.push(v))\n",
    "\n",
    "    def swap(self, i, j):\n",
    "        assert i < self.val.end\n",
    "        assert j < self.val.end\n",
    "        self.val.val[i], self.val.val[j] = self.val.val[j], self.val.val[i]\n",
    "\n",
    "    def up(self, i):\n",
    "        while 1 < i:\n",
    "            p = i // 2\n",
    "            if self.val.val[p] < self.val.val[i]:\n",
    "                self.swap(i, p)\n",
    "                i = p\n",
    "            else:\n",
    "                break\n",
    "        return i\n",
    "\n",
    "    def down(self, i):\n",
    "        n = self.val.end\n",
    "        while 2*i < n:\n",
    "            a = 2 * i\n",
    "            b = 2 * i + 1\n",
    "            c = i\n",
    "            if self.val.val[c] < self.val.val[a]:\n",
    "                c = a\n",
    "            if b < n and self.val.val[c] < self.val.val[b]:\n",
    "                c = b\n",
    "            if c == i:\n",
    "                break\n",
    "            self.swap(i, c)\n",
    "            i = c\n",
    "        return i\n",
    "\n",
    "    def _update(self, i, old, new):\n",
    "        assert i < self.val.end\n",
    "        if old == new: return i   # value unchanged\n",
    "        self.val.val[i] = new         # perform change\n",
    "        if old < new:             # increased\n",
    "            return self.up(i)\n",
    "        else:                     # decreased\n",
    "            return self.down(i)\n",
    "\n",
    "    def _remove(self, i):\n",
    "        # update the locator stuff for last -> i\n",
    "        last = self.val.end - 1\n",
    "        self.swap(i, last)\n",
    "        old = self.val.pop()\n",
    "        # special handling for when the heap has size one.\n",
    "        if i == last: return\n",
    "        self._update(i, old, self.val.val[i])\n",
    "\n",
    "    def check(self):\n",
    "        # heap property\n",
    "        for i in range(2, self.val.end):\n",
    "            assert self.val[i] <= self.val[i // 2], (self.val[i // 2], self.val[i])   # child <= parent\n",
    "\n",
    "\n",
    "class LocatorMaxHeap(MaxHeap):\n",
    "    \"\"\"\n",
    "    Dynamic heap. Maintains max of a map, via incrementally maintained partial\n",
    "    aggregation tree. Also known a priority queue with 'locators'.\n",
    "    This data structure efficiently maintains maximum of the priorities of a set\n",
    "    of keys. Priorites may increase or decrease. (Many max-heap implementations\n",
    "    only allow increasing priority.)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kw):\n",
    "        super().__init__(**kw)\n",
    "        self.key = {}   # map from index `i` to `key`\n",
    "        self.loc = {}   # map from `key` to index in `val`\n",
    "\n",
    "    def __repr__(self):\n",
    "        return repr({k: self[k] for k in self.loc})\n",
    "\n",
    "    def pop(self):\n",
    "        k,v = self.peek()\n",
    "        super().pop()\n",
    "        return k,v\n",
    "\n",
    "    def popitem(self):\n",
    "        return self.pop()\n",
    "\n",
    "    def peek(self):\n",
    "        return self.key[1], super().peek()\n",
    "\n",
    "    def _remove(self, i):\n",
    "        # update the locator stuff for last -> i\n",
    "        last = self.val.end - 1\n",
    "        self.swap(i, last)\n",
    "        old = self.val.pop()\n",
    "        # remove the key/loc/val associated with the deleted node.\n",
    "        self.loc.pop(self.key.pop(last))\n",
    "        # special handling for when the heap has size one.\n",
    "        if i == last: return\n",
    "        self._update(i, old, self.val.val[i])\n",
    "\n",
    "    def __delitem__(self, k):\n",
    "        self._remove(self.loc[k])\n",
    "\n",
    "    def __contains__(self, k):\n",
    "        return k in self.loc\n",
    "\n",
    "    def __getitem__(self, k):\n",
    "        return self.val.val[self.loc[k]]\n",
    "\n",
    "    def __setitem__(self, k, v):\n",
    "        \"upsert (update or insert) value associated with key.\"\n",
    "        if k in self:\n",
    "            # update\n",
    "            i = self.loc[k]\n",
    "            super()._update(i, self.val[i], v)\n",
    "        else:\n",
    "            # insert (put new element last and bubble up)\n",
    "            i = self.val.push(v)\n",
    "            # Annoyingly, we have to write key/loc here the super class's push\n",
    "            # method doesn't allow us to intervene before the up call.\n",
    "            self.val[i] = v\n",
    "            self.loc[k] = i\n",
    "            self.key[i] = k\n",
    "            # fix invariants\n",
    "            self.up(i)\n",
    "\n",
    "    def swap(self, i, j):\n",
    "        assert i < self.val.end\n",
    "        assert j < self.val.end\n",
    "        self.val.val[i], self.val.val[j] = self.val.val[j], self.val.val[i]\n",
    "\n",
    "        self.key[i], self.key[j] = self.key[j], self.key[i]\n",
    "        self.loc[self.key[i]] = i\n",
    "        self.loc[self.key[j]] = j\n",
    "\n",
    "    def check(self):\n",
    "        super().check()\n",
    "        for key in self.loc:\n",
    "            assert self.key[self.loc[key]] == key\n",
    "        for i in range(1, self.val.end):\n",
    "            assert self.loc[self.key[i]] == i\n",
    "\n",
    "class PriorityQueue:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.heap = LocatorMaxHeap()\n",
    "\n",
    "    def push(self, item, w):\n",
    "            if item in self.heap:\n",
    "                self.heap[item] = max(self.heap[item], float(w))\n",
    "            else:\n",
    "                self.heap[item] = float(w)\n",
    "\n",
    "    def pop(self):\n",
    "        item, score = self.heap.pop()\n",
    "        return item, score\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.heap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queue length: 3\n",
      "((2, 100), 1.3)\n",
      "Queue length: 2\n",
      "((0, 2), 1.1)\n",
      "Queue length: 1\n"
     ]
    }
   ],
   "source": [
    "queue = PriorityQueue()\n",
    "queue.push((0, 1), 0.2)\n",
    "queue.push((0, 2), 1.1)\n",
    "queue.push((0, 2), 0.1)\n",
    "queue.push((2, 100), 1.3)\n",
    "print(\"Queue length:\", len(queue))\n",
    "print(queue.pop())\n",
    "print(\"Queue length:\", len(queue))\n",
    "print(queue.pop())\n",
    "print(\"Queue length:\", len(queue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralCRF(nn.Module):\n",
    "    \"\"\"Class modeling a neural CRF for POS tagging.\n",
    "    We model tag-tag dependencies with a weight for each transition\n",
    "    and word-tag influence through an LSTM on top of a pretrained transformer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        pad_idx_word,\n",
    "        pad_idx_pos,\n",
    "        bos_idx,\n",
    "        eos_idx,\n",
    "        bot_idx,\n",
    "        eot_idx,\n",
    "        t_cal,\n",
    "        transformer,\n",
    "        lstm_hidden_dim=64,\n",
    "        beta=0,\n",
    "    ):\n",
    "        \"\"\"Constructor.\n",
    "\n",
    "        Parameters\n",
    "        ---\n",
    "        pad_idx_word : int\n",
    "            Index corresponding to padding in the word sequences.\n",
    "        pad_idx_pos : int\n",
    "            Index corresponding to padding in the tag sequences.\n",
    "        bos_idx : int\n",
    "            Index corresponding to beginning of speech marker in the word sequences.\n",
    "        eos_idx : int\n",
    "            Index corresponding to end of speech marker in the word sequences.\n",
    "        bot_idx : int\n",
    "            Index corresponding to beginning of tag marker in the tag sequences.\n",
    "        eot_idx : int\n",
    "            Index corresponding to end of tag marker in the tag sequences.\n",
    "        t_cal : List[int]\n",
    "            List containing all indices corresponding to tags in the tag sequences.\n",
    "        transformer : BertModel\n",
    "            Pretrained transformer used to embed sentences before feeding them\n",
    "            into the LSTM.\n",
    "        lstm_hiden_dim : int\n",
    "            Hidden dimension of the LSTM used for POS tagging. Note that\n",
    "            since we are bidirectional, the effective hidden dimension\n",
    "            is half of this number.\n",
    "        beta : float\n",
    "            Regularization hyperparameter of the entropy regularizer.\n",
    "            Entropy regularization is only applied for \\beta > 0.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.pad_idx_word = pad_idx_word\n",
    "        self.pad_idx_pos = pad_idx_pos\n",
    "        self.bos_idx = bos_idx\n",
    "        self.eos_idx = eos_idx\n",
    "        self.bot_idx = bot_idx\n",
    "        self.eot_idx = eot_idx\n",
    "        self.t_cal = t_cal\n",
    "        self.transformer = transformer\n",
    "        self.lstm_hidden_dim = lstm_hidden_dim\n",
    "        self.beta = beta\n",
    "        self.transitions = nn.Parameter(torch.empty(len(t_cal), len(t_cal)))\n",
    "        self.emissions = TagLSTM(\n",
    "            len(t_cal),\n",
    "            transformer.config.to_dict()[\"hidden_size\"],\n",
    "            lstm_hidden_dim,\n",
    "        )\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        nn.init.uniform_(self.transitions, -0.1, 0.1)\n",
    "\n",
    "    def forward(self, W):\n",
    "        \"\"\"Decode each sentence within W and return predicted tagging.\n",
    "\n",
    "        Parameters\n",
    "        ---\n",
    "        W : torch.tensor\n",
    "            Word sequences of dimension batch size x max sentence length within batch + 2.\n",
    "\n",
    "        Returns\n",
    "        ---\n",
    "        sequences : list\n",
    "            List of tensors, each of which contains the predicted tag indices for a particular\n",
    "            word sequence.\n",
    "        \"\"\"\n",
    "        # Calculate scores.\n",
    "        emissions = self.calculate_emissions(W)\n",
    "        # Run viterbi sentence by sentence.\n",
    "        sequences = []\n",
    "        for sentence in range(W.shape[0]):\n",
    "            # Exclude beginning and end markers from each word sequence.\n",
    "            scores, backpointers = self.backward_viterbi_log(\n",
    "                W[sentence, 1:], emissions[sentence, :]\n",
    "            )\n",
    "            sequences = sequences + [self.get_viterbi(backpointers)]\n",
    "        return sequences\n",
    "\n",
    "    def calculate_emissions(self, W):\n",
    "        \"\"\"Calculate emissions (i.e., scores for each word and batch).\n",
    "\n",
    "        Parameters\n",
    "        ---\n",
    "        W : torch.tensor\n",
    "            Word sequences of dimension batch size x max sentence\n",
    "            length within batch + 2.\n",
    "\n",
    "        Returns\n",
    "        ---\n",
    "        emissions : torch.tensor\n",
    "            Word level scores for each tag of dimension batch_size x max\n",
    "            sentence length within batch + 1 x |T|.\n",
    "            The scores for the initial BOS index are already removed here\n",
    "            since we only needed it for the transformer.\n",
    "        \"\"\"\n",
    "        # Directly exclude emissions for the initial word in each sentence\n",
    "        # since these correspond to the BOS indices that we only need\n",
    "        # for BERT.\n",
    "        return self.emissions(self.transformer(W)[0])[:, 1:, :]\n",
    "\n",
    "    def loss(self, T, W):\n",
    "        \"\"\"Calculate the loss for a batch.\n",
    "\n",
    "        Parameters\n",
    "        ---\n",
    "        T : torch.tensor\n",
    "            True taggings for each sequence within the batch.\n",
    "            Of dimension batch size x longest sequence within batch + 2.\n",
    "            Note the paddings, EOS and BOS that have been added to T\n",
    "            for symmetry with W which needs this for BERT.\n",
    "        W : torch.tensor\n",
    "            Words for each sequence within the batch.\n",
    "            Of dimension batch size x longest sequence within batch + 2.\n",
    "            Note the paddings, EOS and BOS that have been added to W\n",
    "            for usage with BERT.\n",
    "\n",
    "        Returns\n",
    "        ---\n",
    "        torch.tensor\n",
    "            Mean loss for the batch.\n",
    "        \"\"\"\n",
    "        emissions = self.calculate_emissions(W)\n",
    "        # Note that we have to handle paddings and EOS within the score\n",
    "        # and backward functions, but we can already skip the BOS tokens\n",
    "        # here.\n",
    "        scores = self.score(emissions, W[:, 1:], T[:, 1:])\n",
    "        log_normalizer = self.backward_log_Z(W[:, 1:], emissions)\n",
    "        loss = torch.negative(torch.mean(scores - log_normalizer))\n",
    "        if self.beta > 0.0:\n",
    "            unnormalized_entropy = self.backward_entropy(\n",
    "                W[:, 1:], emissions\n",
    "            )\n",
    "            entropy = (\n",
    "                (unnormalized_entropy / torch.exp(log_normalizer))\n",
    "                + log_normalizer\n",
    "            )\n",
    "            return loss + torch.negative(self.beta * torch.mean(entropy))\n",
    "        else:\n",
    "            return loss\n",
    "\n",
    "    def score(self, emissions, W, T):\n",
    "        \"\"\"Calculate scores for specified taggings and word sequences.\n",
    "\n",
    "        Parameters\n",
    "        ---\n",
    "        emissions : torch.tensor\n",
    "        T : torch.tensor\n",
    "            Taggings for each sequence within the batch.\n",
    "            Of dimension batch size x longest sequence within batch + 1.\n",
    "            Note the paddings, EOS and BOS that have been added to T\n",
    "            for symmetry with W which needs this for BERT.\n",
    "            We expect T to already have the initial BOT tag indices removed\n",
    "            (see `loss` for details).\n",
    "        W : torch.tensor\n",
    "            Words for each sequence within the batch.\n",
    "            Of dimension batch size x longest sequence within batch + 1.\n",
    "            Note the paddings, EOS and BOS that have been added to W\n",
    "            for usage with BERT so we mask them out here. We expect\n",
    "            W to already have the initial BOS word indices taken out\n",
    "            (see `loss` for details).\n",
    "\n",
    "        Returns\n",
    "        ---\n",
    "        scores : torch.tensor\n",
    "            score(T, W) for all samples in W.\n",
    "        \"\"\"\n",
    "        scores = (\n",
    "            emissions[:, 0].gather(1, (T[:, 0]).unsqueeze(1)).squeeze()\n",
    "            + self.transitions[self.bot_idx, T[:, 0]]\n",
    "        )\n",
    "        for word in range(1, emissions.shape[1]):\n",
    "            mask = torch.where(\n",
    "                W[:, word] == self.pad_idx_word, 0, 1\n",
    "            ) * torch.where(W[:, word] == self.eos_idx, 0, 1)\n",
    "            scores = scores + mask * (\n",
    "                emissions[:, word]\n",
    "                .gather(1, (T[:, word]).unsqueeze(1))\n",
    "                .squeeze()\n",
    "                + self.transitions[T[:, word - 1], T[:, word]]\n",
    "            )\n",
    "        return scores\n",
    "\n",
    "    def viterbi_naive(self, W, emissions):\n",
    "        \"\"\"Calculate best tagging naively and return both the best score and best tagging in log space.\n",
    "\n",
    "        NB: This naive version is not vectorized over samples.\n",
    "\n",
    "        Parameters\n",
    "        ---\n",
    "        W : torch.tensor\n",
    "            Of dimension longest sequence within batch + 2 or less.\n",
    "            Note the paddings, EOS and BOS that have been added to W\n",
    "            for usage with BERT so we manually remove them here if present.\n",
    "        emissions : torch.tensor\n",
    "            Word level scores for each tag of dimension max\n",
    "            sentence length within batch + 1 x |T| (we assume scores for the BOT\n",
    "            initial tag have already been removed since BOT/BOS is\n",
    "            only needed for the transformer).\n",
    "\n",
    "        Returns\n",
    "        ---\n",
    "        Tuple[torch.tensor, torch.tensor]\n",
    "            Tuple containing the log-score of the best tagging and the\n",
    "            indices of the best tagging for W.\n",
    "        \"\"\"\n",
    "        T = self.t_cal\n",
    "        # Remove padding.\n",
    "        if torch.any(W == self.pad_idx_word):\n",
    "            W = W[torch.where(W != self.pad_idx_word)[0]]\n",
    "        # Remove EOS and BOS if present.\n",
    "        if torch.any(W == self.eos_idx):\n",
    "            W = W[:-1]\n",
    "        if torch.any(W == self.bos_idx):\n",
    "            W = W[1:]\n",
    "        T_abs = len(T)\n",
    "        combinations = torch.combinations(\n",
    "            T, r=W.shape[0], with_replacement=True\n",
    "        )\n",
    "        combinations = torch.cartesian_prod(*[T for ix in range(W.shape[0])])\n",
    "        best_score = torch.tensor(0.0, dtype=torch.float64)\n",
    "        best_tag = torch.tensor([])\n",
    "        for ix, combination in enumerate(combinations):\n",
    "            if W.shape[0] == 1:\n",
    "                current_score = (\n",
    "                    emissions[0, combination]\n",
    "                    + self.transitions[self.bot_idx, combination]\n",
    "                )\n",
    "            else:\n",
    "                current_score = (\n",
    "                    emissions[0, combination[0]]\n",
    "                    + self.transitions[self.bot_idx, combination[0]]\n",
    "                )\n",
    "                for qx in range(1, combination.shape[0]):\n",
    "                    current_score = current_score + emissions[qx, combination[qx]] + self.transitions[combination[qx - 1], combination[qx]]\n",
    "\n",
    "            if (current_score) > best_score:\n",
    "                best_score = current_score.double()\n",
    "                best_tag = combination\n",
    "        return best_score, best_tag\n",
    "\n",
    "    def log_Z_naive(self, W, emissions):\n",
    "        \"\"\"Calculate log Z naively.\n",
    "\n",
    "        NB: This naive version is not vectorized over samples.\n",
    "\n",
    "        Parameters\n",
    "        ---\n",
    "        W : torch.tensor\n",
    "            Of dimension longest sequence within batch + 2 or less.\n",
    "            Note the paddings, EOS and BOS that have been added to W\n",
    "            for usage with BERT so we manually remove them here if present.\n",
    "        emissions : torch.tensor\n",
    "            Word level scores for each tag of dimension max\n",
    "            sentence length within batch + 1 x |T| (we assume scores for the BOT\n",
    "            initial tag have already been removed since BOT/BOS is\n",
    "            only needed for the transformer).\n",
    "\n",
    "        Returns\n",
    "        ---\n",
    "        torch.tensor\n",
    "            Log Z for W.\n",
    "        \"\"\"\n",
    "        T = self.t_cal\n",
    "        # Remove padding\n",
    "        W = W[torch.where(W != self.pad_idx_word)[0]]\n",
    "        # Remove EOS and BOS if present\n",
    "        if torch.any(W == self.eos_idx):\n",
    "            W = W[:-1]\n",
    "        if torch.any(W == self.bos_idx):\n",
    "            W = W[1:]\n",
    "        T_abs = len(T)\n",
    "        \n",
    "        # Generate \\mathcal{T}^N.\n",
    "        combinations = torch.cartesian_prod(*[T for ix in range(W.shape[0])])\n",
    "        log_normalizer = torch.zeros(\n",
    "            combinations.shape[0], dtype=torch.float64\n",
    "        )\n",
    "        # Loop over all possible combinations naively.\n",
    "        # NB: This is essentially line one on Slide 50.\n",
    "        # print(\"Combinations\", combinations.shape, combinations)\n",
    "        # print(\"T\", T.shape, T)\n",
    "        # print(\"W\", W.shape, W)\n",
    "        for ix, combination in enumerate(combinations):\n",
    "            # Kludge since indexing is slightly different for one-dim\n",
    "            # tensors vs two tensors.\n",
    "            if W.shape[0] == 1:\n",
    "                # Calculate score as the sum of emissions (i.e., how well\n",
    "                # does a word match a tag based on BERT embeddings) and\n",
    "                # transitions (globally, how likely is a transition\n",
    "                # from the previous tag to the current tag).\n",
    "                # NB: For the first word, the initial tag is always BOT.\n",
    "                # NB 2: Since we are in log-space, the exp\n",
    "                # of the score goes away.\n",
    "                log_normalizer[ix] = (\n",
    "                    emissions[0, combination]\n",
    "                    + self.transitions[self.bot_idx, combination]\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                #-------------------------------------------------------\n",
    "                # INITALIZE THE STARTING VALUES\n",
    "                #-------------------------------------------------------\n",
    "                # Initial score is identical to above.\n",
    "                log_normalizer[ix] = (\n",
    "                    emissions[0, combination[0]]\n",
    "                    + self.transitions[self.bot_idx, combination[0]]\n",
    "                )\n",
    "                for qx in range(1, combination.shape[0]):\n",
    "                    # Score within each potential tagging\n",
    "                    # is calculated the same as above except that we now \n",
    "                    # actually use the previous tag instead of always\n",
    "                    # BOT.\n",
    "                #-------------------------------------------------------\n",
    "                # SUM THE SCORES ALONG THE EDGES OF THE TAGGING\n",
    "                #-------------------------------------------------------\n",
    "                    log_normalizer[ix] = log_normalizer[ix] + emissions[qx, combination[qx]] + self.transitions[combination[qx - 1], combination[qx]]\n",
    "        # Calculate logsumexp numerically stable\n",
    "        # since we are in log-space.\n",
    "        # print(\"log_normalizer\", log_normalizer.shape, log_normalizer)\n",
    "\n",
    "        return torch.logsumexp(log_normalizer, 0)\n",
    "\n",
    "\n",
    "    def entropy_naive(self, W, emissions):\n",
    "        \"\"\"Calculate the unnormalized entropy naively.\n",
    "\n",
    "        NB: This naive version is not vectorized over samples.\n",
    "\n",
    "        Parameters\n",
    "        ---\n",
    "        W : torch.tensor\n",
    "            Words for each sequence within the batch.\n",
    "            Of dimension longest sequence within batch + 2 or less.\n",
    "            Note the paddings, EOS and BOS that have been added to W\n",
    "            for usage with BERT so we manually remove them here if present.\n",
    "        emissions : torch.tensor\n",
    "            Word level scores for each tag of dimension max\n",
    "            sentence length within batch + 1 x |T| (we assume scores for the BOT\n",
    "            initial tag have already been removed since BOT/BOS is\n",
    "            only needed for the transformer).\n",
    "\n",
    "        Returns\n",
    "        ---\n",
    "        torch.tensor\n",
    "            Log Z for W.\n",
    "        \"\"\"\n",
    "        T = self.t_cal\n",
    "        # Remove padding\n",
    "        W = W[torch.where(W != self.pad_idx_word)[0]]\n",
    "        # Remove EOS and BOS if present\n",
    "        if torch.any(W == self.eos_idx):\n",
    "            W = W[:-1]\n",
    "        if torch.any(W == self.bos_idx):\n",
    "            W = W[1:]\n",
    "        T_abs = len(T)\n",
    "        combinations = torch.combinations(\n",
    "            T, r=W.shape[0], with_replacement=True\n",
    "        )\n",
    "        combinations = torch.cartesian_prod(T, T)\n",
    "        combinations = torch.cartesian_prod(*[T for ix in range(W.shape[0])])\n",
    "        entropy = torch.zeros(1, dtype=torch.float64)\n",
    "        for ix, combination in enumerate(combinations):\n",
    "            if W.shape[0] == 1:\n",
    "                entropy -= torch.exp((\n",
    "                    emissions[0, combination]\n",
    "                    + self.transitions[self.bot_idx, combination]\n",
    "                )) * (\n",
    "                    emissions[0, combination]\n",
    "                    + self.transitions[self.bot_idx, combination]\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                local_score = (\n",
    "                    emissions[0, combination[0]]\n",
    "                    + self.transitions[self.bot_idx, combination[0]]\n",
    "                )\n",
    "                for qx in range(1, combination.shape[0]):\n",
    "                    local_score = local_score + (\n",
    "                        emissions[qx, combination[qx]]\n",
    "                        + self.transitions[\n",
    "                            combination[qx - 1], combination[qx]\n",
    "                        ]\n",
    "                    )\n",
    "                entropy -= torch.exp(local_score) * local_score\n",
    "        return entropy\n",
    "\n",
    "    def get_viterbi(self, backpointer_matrix):\n",
    "        \"\"\"Return the best tagging based on a backpointer matrix.\n",
    "\n",
    "        Parameters\n",
    "        ---\n",
    "        backpointer_matrix : torch.tensor\n",
    "            Backpointer matrix from Viterbi indicating which\n",
    "            tag is the highest scoring for each element in the sequence.\n",
    "\n",
    "        Returns\n",
    "        ---\n",
    "        torch.tensor\n",
    "            Indices of the best tagging based on `backpointer_matrix`.\n",
    "        \"\"\"\n",
    "        N = backpointer_matrix.shape[0]\n",
    "        t = torch.zeros(N, dtype=torch.int64)\n",
    "        t[0] = backpointer_matrix[0, self.bot_idx]\n",
    "        for n in range(1, N):\n",
    "            t[n] = backpointer_matrix[n, t[n-1].item()]\n",
    "        return t\n",
    "\n",
    "    def backward_log_Z(self, W, emissions):\n",
    "        \"\"\"Calculate log Z using the backward algorithm.\n",
    "\n",
    "        NB: You do need to vectorize this over samples.\n",
    "\n",
    "        Parameters\n",
    "        ---\n",
    "        W : torch.tensor\n",
    "            Words for each sequence within the batch.\n",
    "            Of dimension batch size x longest sequence within batch + 1.\n",
    "            Note the paddings, EOS and BOS have been added to W\n",
    "            for usage with BERT so we mask them out here. We expect\n",
    "            W to already have the initial BOS word indices taken out.\n",
    "        emissions : torch.tensor\n",
    "            Word level scores for each tag of dimension batch_size x max\n",
    "            sentence length within batch + 1 x |T| (scores for the BOS\n",
    "            initial tag have already been removed since BOS is\n",
    "            only needed for the transformer).\n",
    "\n",
    "        Returns\n",
    "        ---\n",
    "        torch.tensor\n",
    "            Log Z for each sample in W.\n",
    "        \"\"\"\n",
    "        #---------------------------------------------------------------------------------------------------------------\n",
    "        #>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> NON-VECTORIZED OVER SAMPLES <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "        #---------------------------------------------------------------------------------------------------------------\n",
    "        # T = self.t_cal\n",
    "        # log_Z = torch.zeros(W.shape[0], dtype=torch.float64)\n",
    "        # for sentence_idx, sentence in enumerate(W):\n",
    "        #     # Remove padding from sentence\n",
    "        #     sentence = sentence[torch.where(sentence != self.pad_idx_word)]\n",
    "        #     if torch.any(sentence == self.eos_idx):\n",
    "        #         sentence = sentence[:-1]\n",
    "        #     N = len(sentence) + 1\n",
    "        #     beta = torch.zeros((N, len(T)), dtype=torch.float64)\n",
    "        #     # Initialize EOT scores\n",
    "        #     beta[N-1, :] = 1.0\n",
    "        #     # Propagate backwards values from layer to layer\n",
    "        #     for n in range(N-2, 0, -1):\n",
    "        #         beta[n, :] = torch.sum(torch.exp(emissions[sentence_idx, n, :] + self.transitions[:, :]) * beta[n+1, :], 1)\n",
    "\n",
    "        #     # Handle BOT edge case\n",
    "        #     Z = torch.sum(torch.exp(emissions[sentence_idx, 0, :] + self.transitions[0, :]) * beta[1, :])\n",
    "        #     log_Z[sentence_idx] = torch.log(Z)\n",
    "\n",
    "        # return log_Z\n",
    "        #---------------------------------------------------------------------------------------------------------------\n",
    "        #>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> VECTORIZED OVER SAMPLES <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "        #---------------------------------------------------------------------------------------------------------------\n",
    "        T = self.t_cal\n",
    "        # Mask to mask over EOS, BOS and PAD words\n",
    "        M = ((W != self.bos_idx) & (W != self.eos_idx) & (W != self.pad_idx_word)).long()\n",
    "        num_samples, N = W.shape[0], W.shape[1] + 1\n",
    "        log_Z = torch.zeros(num_samples, dtype=torch.float64)\n",
    "        beta = torch.zeros((num_samples, N, len(T)), dtype=torch.float64)\n",
    "        # Initialize EOT scores\n",
    "        beta[:, N-1, :] = 1.0\n",
    "        # Propagate backwards values from layer to layer\n",
    "        for n in range(N-2, 0, -1):\n",
    "            for t_n in T:\n",
    "                beta[:, n, t_n] = torch.sum(torch.exp(emissions[:, n, :] + self.transitions[t_n, :]) * beta[:, n + 1, :].clone(), axis=1) * M[:, n] + (1 - M[:, n]) * beta[:, n + 1, t_n].clone()\n",
    "\n",
    "        # Handle BOT edge case\n",
    "        return torch.log(torch.sum(torch.exp(emissions[:, 0, :] + self.transitions[self.bot_idx, :]) * beta[:, 1, :].clone(), axis=1))\n",
    "\n",
    "\n",
    "    def forward_log_Z(self, W, emissions):\n",
    "        \"\"\"Calculate log Z using the forward algorithm.\n",
    "\n",
    "        NB: You do need to vectorize this over samples.\n",
    "\n",
    "        Parameters\n",
    "        ---\n",
    "        W : torch.tensor\n",
    "            Words for each sequence within the batch.\n",
    "            Of dimension batch size x longest sequence within batch + 1.\n",
    "            Note the paddings, EOS and BOS that have been added to W\n",
    "            for usage with BERT so we mask them out here. We expect\n",
    "            W to already have the initial BOS word indices taken out.\n",
    "        emissions : torch.tensor\n",
    "            Word level scores for each tag of dimension batch_size x max\n",
    "            sentence length within batch + 1 x |T| (scores for the BOS\n",
    "            initial tag have already been removed since BOS is\n",
    "            only needed for the transformer).\n",
    "\n",
    "        Returns\n",
    "        ---\n",
    "        torch.tensor\n",
    "            Log Z for each sample in W.\n",
    "        \"\"\"\n",
    "        T = self.t_cal\n",
    "        # Remove BOS from samples if present\n",
    "        if torch.any(W == self.bos_idx):\n",
    "            W = W[1:]\n",
    "        # Mask to mask over EOS, BOS and PAD words\n",
    "        M = ((W != self.bos_idx) & (W != self.eos_idx) & (W != self.pad_idx_word)).long()\n",
    "        num_samples, N = W.shape\n",
    "        # log_Z = torch.zeros(num_samples, dtype=torch.float64)\n",
    "        gamma = torch.zeros((num_samples, N, len(T)), dtype=torch.float64)\n",
    "        # Initialize first layer scores\n",
    "        for t_0 in T:\n",
    "            gamma[:, 0, t_0] = torch.exp(emissions[:, 0, t_0] + self.transitions[self.bot_idx, t_0])\n",
    "        # Propagate forward values from layer to layer\n",
    "        for n in range(1, N-1):\n",
    "            for t_n in T:\n",
    "                gamma[:, n, t_n] = torch.sum(\n",
    "                    torch.exp(emissions[:, n, t_n].reshape((-1, 1)) + self.transitions[:, t_n]) \n",
    "                    * gamma[:, n-1, :].clone(), dim=1\n",
    "                    ) * M[:, n] + (1 - M[:, n]) * gamma[:, n-1, t_n].clone()\n",
    "\n",
    "        # Handle EOT edge case\n",
    "        # return torch.log(torch.sum(torch.exp(emissions[:, N-1, self.eot_idx].reshape((-1, 1)) + self.transitions[:, self.eot_idx]) * gamma[:, N-2, :], dim=1))\n",
    "        return torch.log(torch.sum(gamma[:, N-2, :].clone(), dim=1))\n",
    "\n",
    "\n",
    "    def backward_entropy(self, W, emissions):\n",
    "        \"\"\"Calculate the unnormalized entropy using the backward algorithm.\n",
    "\n",
    "        NB: You do need to vectorize this over samples.\n",
    "\n",
    "        Parameters\n",
    "        ---\n",
    "        W : torch.tensor\n",
    "            Words for each sequence within the batch.\n",
    "            Of dimension batch size x longest sequence within batch + 1.\n",
    "            Note the paddings, EOS and BOS that have been added to W\n",
    "            for usage with BERT so we mask them out here. We expect\n",
    "            W to already have the initial BOS word indices taken out.\n",
    "        emissions : torch.tensor\n",
    "            Word level scores for each tag of dimension batch_size x max\n",
    "            sentence length within batch + 1 x |T| (scores for the EOS\n",
    "            initial tag have already been removed since EOS is\n",
    "            only needed for the transformer).\n",
    "\n",
    "        Returns\n",
    "        ---\n",
    "        torch.tensor\n",
    "            Unnormalized entropy for each sample in W.\n",
    "        \"\"\"\n",
    "        T = self.t_cal\n",
    "        # Mask to mask over EOS, BOS and PAD words\n",
    "        M = ((W != self.bos_idx) & (W != self.eos_idx) & (W != self.pad_idx_word)).long()\n",
    "        num_samples, N = W.shape[0], W.shape[1] + 1\n",
    "        beta = torch.zeros((num_samples, N, len(T), 2), dtype=torch.float64)\n",
    "        # Initialize EOT scores\n",
    "        w = torch.tensor(1.0)\n",
    "        beta[:, N-1, :, 0] = w\n",
    "        beta[:, N-1, :, 1] = -w * torch.log(w)\n",
    "        # Propagate backwards values from layer to layer\n",
    "        for n in range(N-2, 0, -1):\n",
    "            for t_n in T:\n",
    "                w = torch.exp(emissions[:, n, :] + self.transitions[t_n, :])\n",
    "                beta[:, n, t_n, 0] = torch.sum(w * beta[:, n + 1, :, 0].clone(), axis=1) * M[:, n] + (1 - M[:, n]) * beta[:, n + 1, t_n, 0].clone()\n",
    "                beta[:, n, t_n, 1] = torch.sum(w * beta[:, n + 1, :, 1].clone() - w * torch.log(w) * beta[:, n + 1, :, 0].clone(), axis=1) * M[:, n] + (1 - M[:, n]) * beta[:, n + 1, t_n, 1].clone()\n",
    "\n",
    "        # Handle BOT edge case\n",
    "        w = torch.exp(emissions[:, 0, :] + self.transitions[self.bot_idx, :])\n",
    "        return torch.sum(w * beta[:, 1, :, 1].clone() - w * torch.log(w) * beta[:, 1, :, 0].clone(), axis=1)\n",
    "\n",
    "    def backward_viterbi_log(self, W, emissions):\n",
    "        \"\"\"Calculate the best tagging using the backward algorithm and return\n",
    "            both the scoring matrix in log-space and the backpointer matrix.\n",
    "\n",
    "        NB: You do not need to vectorize this over samples.\n",
    "\n",
    "        Parameters\n",
    "        ---\n",
    "        W : torch.tensor\n",
    "            Of dimension longest sequence within batch + 2 or less.\n",
    "            Note the paddings, EOS and BOS that have been added to W\n",
    "            for usage with BERT so we manually remove them here if present.\n",
    "        emissions : torch.tensor\n",
    "            Word level scores for each tag of dimension max\n",
    "            sentence length within batch + 1 x |T| (we assume scores for the BOT\n",
    "            initial tag have already been removed since BOT/BOS is\n",
    "            only needed for the transformer).\n",
    "\n",
    "        Returns\n",
    "        ---\n",
    "        Tuple[torch.tensor, torch.tensor]\n",
    "            Tuple containing the scoring matrix in log-space and the\n",
    "            backpointer matrix for recovering the best tagging.\n",
    "        \"\"\"\n",
    "        T = self.t_cal\n",
    "        if W.ndim == 1:\n",
    "            W = W[None, :]\n",
    "        log_score_viterbi = torch.zeros(W.shape[0], dtype=torch.float64)\n",
    "        emissions = emissions[None, :]\n",
    "        for sentence_idx, sentence in enumerate(W):\n",
    "            # Remove padding from sentence\n",
    "            sentence = sentence[torch.where(sentence != self.pad_idx_word)]\n",
    "            if torch.any(sentence == self.eos_idx):\n",
    "                sentence = sentence[:-1]\n",
    "            if torch.any(sentence == self.bos_idx):\n",
    "                sentence = sentence[1:]\n",
    "            N = len(sentence) + 1\n",
    "            beta = torch.zeros((N, len(T)), dtype=torch.float64)\n",
    "            backpointers = torch.zeros((N-1, len(T)), dtype=torch.float64)\n",
    "            # Initialize EOT scores\n",
    "            beta[N-1, :] = 1.0\n",
    "            # Propagate backwards values from layer to layer\n",
    "            for n in range(N-2, 0, -1):\n",
    "                values, argmax = torch.max(torch.exp(emissions[sentence_idx, n, :] + self.transitions[:, :]) * beta[n+1, :].clone(), dim=1)\n",
    "                beta[n, :] = values\n",
    "                backpointers[n, :] = argmax\n",
    "\n",
    "            # Handle BOT edge case\n",
    "            score_viterbi, argmax_t_1 = torch.max((torch.exp(emissions[sentence_idx, 0, :] + self.transitions[self.bot_idx, :]) * beta[1, :].clone())[None, :], dim=1)\n",
    "            backpointers[0, :] = argmax_t_1\n",
    "            log_score_viterbi[sentence_idx] = torch.log(score_viterbi)\n",
    "\n",
    "        return log_score_viterbi.reshape((1, -1)), backpointers\n",
    "\n",
    "    def dijkstra_viterbi_log(self, W, emissions):\n",
    "        \"\"\"Calculate the best tagging using Dijsktra's algorithm and return\n",
    "            both the best score and best tagging in log space.\n",
    "\n",
    "        NB: You do not need to vectorize this over samples.\n",
    "\n",
    "        Parameters\n",
    "        ---\n",
    "        W : torch.tensor\n",
    "            Of dimension longest sequence within batch + 2 or less.\n",
    "            Note the paddings, EOS and BOS that have been added to W\n",
    "            for usage with BERT so we manually remove them here if present.\n",
    "        emissions : torch.tensor\n",
    "            Word level scores for each tag of dimension max\n",
    "            sentence length within batch + 1 x |T| (we assume scores for the BOT\n",
    "            initial tag have already been removed since BOT/BOS is\n",
    "            only needed for the transformer).\n",
    "\n",
    "\n",
    "        Returns\n",
    "        ---\n",
    "        Tuple[torch.tensor, None, log_Z]\n",
    "            Tuple containing the log-score of the best tagging. \n",
    "            NB: Since there were some changes in the assignment,\n",
    "            we don't expect you to return the backpointer matrix\n",
    "            this year.\n",
    "            NB 2: We return log_Z if we already use it within the method\n",
    "            to calculate probabilities, such that we don't have to\n",
    "        \"\"\"\n",
    "        T = self.t_cal\n",
    "        if W.ndim == 1:\n",
    "            W = W[None, :]\n",
    "        if emissions.ndim == 2:\n",
    "            emissions = emissions[None, :, :]\n",
    "        log_Z = self.backward_log_Z(W, emissions)\n",
    "        score_dijkstra = torch.zeros_like(log_Z)\n",
    "        for sentence_idx, sentence in enumerate(W):\n",
    "            sentence_emissions = emissions[sentence_idx, :, :]\n",
    "            # Remove padding from sentence\n",
    "            sentence = sentence[torch.where(sentence != self.pad_idx_word)]\n",
    "            if torch.any(sentence == self.eos_idx):\n",
    "                sentence = sentence[:-1]\n",
    "            if torch.any(sentence == self.bos_idx):\n",
    "                sentence = sentence[1:]\n",
    "            N = len(sentence)\n",
    "            popped = []\n",
    "            gamma = torch.zeros((N+1, len(T)), dtype=torch.float64)\n",
    "            queue = PriorityQueue()\n",
    "            one = 0.0  # Semi-ring one\n",
    "            queue.push((0, self.bot_idx), one)\n",
    "            while len(queue) > 0:\n",
    "                (n, t), w = queue.pop()\n",
    "                popped.append((n, t))\n",
    "                gamma[n, t] = w\n",
    "                if n == N:  # Early stopping condition\n",
    "                    score_dijkstra[sentence_idx] = w\n",
    "                    break\n",
    "                else:\n",
    "                    for t_prime in T:\n",
    "                        if (n+1, t_prime) not in popped:\n",
    "                            w = sentence_emissions[n, t_prime] + self.transitions[t, t_prime] - log_Z[sentence_idx]\n",
    "                            queue.push((n+1, t_prime), w + gamma[n, t].clone())\n",
    "                \n",
    "        return score_dijkstra, None, log_Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101,
     "referenced_widgets": [
      "8471d000ef1945969fec179cbe2e88f9",
      "6fcafafc3d2e4bdbad2839393c573fad",
      "aef9faf0373b4386817e0999df750166",
      "9ecda6f850bb4bac82fe48923853c3ff",
      "77c97e95b2c2403180bf2416029709cc",
      "2c4e9e0c563f49c890e340e9b5f27f7b",
      "5e655e51ceac40f2b622d3c729ef4076",
      "c4b4b9e02c184a38824e57dee94ae7d3",
      "8b8b98e85c414bfebd0afa36876447c4",
      "51d29018f38c493a823622403fb69f6c",
      "2341945bb058475d9854553f331a088b"
     ]
    },
    "id": "quoz0jBNb26e",
    "outputId": "c7f5f7cd-52ed-4d41-d405-4713ab566666"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(train_dataloader):\n",
    "    W_train = F.to_tensor(data[\"words\"], padding_value=pad_token_idx)\n",
    "    T_train = F.to_tensor(data[\"pos\"], padding_value=pos_vocab[pad_token])\n",
    "    if i == 0:\n",
    "        break\n",
    "\n",
    "bert = BertModel.from_pretrained(TRANSFORMER)\n",
    "T_CAL = torch.tensor([i for i in range(pos_vocab.__len__())])\n",
    "crf = NeuralCRF(\n",
    "    pad_idx_word=pad_token_idx,\n",
    "    pad_idx_pos=pos_vocab[pad_token],\n",
    "    bos_idx=init_token_idx,\n",
    "    eos_idx=sep_token_idx,\n",
    "    bot_idx=pos_vocab[init_token],\n",
    "    eot_idx=pos_vocab[sep_token],\n",
    "    t_cal=T_CAL,\n",
    "    transformer=bert,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bl-a_j9oemGI"
   },
   "source": [
    "## Q3a)\n",
    "----\n",
    "\n",
    "Check that our implementation of backward_log_Z calculates the log normalizer the same\n",
    "as the naive implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "id": "_MqYdBcMeko3",
    "outputId": "08c86c34-dede-4f65-c19c-b5a91b409254"
   },
   "outputs": [],
   "source": [
    "emissions = crf.calculate_emissions(W_train)\n",
    "\n",
    "for sentence_idx in range(W_train.shape[0]):\n",
    "    for word_idx in [2, 3, 4]:\n",
    "        assert torch.isclose(\n",
    "            crf.log_Z_naive(\n",
    "                W_train[sentence_idx, :word_idx],\n",
    "                emissions[\n",
    "                    sentence_idx,\n",
    "                ],\n",
    "            ),\n",
    "            crf.backward_log_Z(W_train[:, 1:word_idx], emissions)[sentence_idx],\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "jfi7hVAKNfRF"
   },
   "outputs": [],
   "source": [
    "# NB: THIS TEST IS NOT GRADED! It is just to help you double check \n",
    "# your implementation of backward since the above test cases do not \n",
    "# contain instances of EOT/PAD.\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "crf = NeuralCRF(\n",
    "    pad_idx_word=pad_token_idx,\n",
    "    pad_idx_pos=pos_vocab[pad_token],\n",
    "    bos_idx=init_token_idx,\n",
    "    eos_idx=sep_token_idx,\n",
    "    bot_idx=pos_vocab[init_token],\n",
    "    eot_idx=pos_vocab[sep_token],\n",
    "    t_cal=T_CAL,\n",
    "    transformer=bert,\n",
    ")\n",
    "emissions = crf.calculate_emissions(W_train)\n",
    "assert torch.all(torch.isclose(crf.backward_log_Z(W_train[:, 1:], emissions), \n",
    "        \n",
    "        torch.tensor([ 88.3197,  54.9441,  51.8413,  49.4724, 110.4930,  40.2174,  40.0351,\n",
    "         49.2687, 107.3545,  60.9668,  58.0769,  89.1132,  57.9029,  51.7252,\n",
    "         83.1715,  79.5805,  82.7579,  33.7521,  61.2349,  27.4993,  45.7138,\n",
    "         49.2352,  52.6344, 122.5854, 150.2306, 156.5732,  40.0482,  30.7149,\n",
    "         27.3859,  92.0778,  61.3652,  80.0837], dtype=torch.float64)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BdHg2VJUt5VY"
   },
   "source": [
    "## Q3b)\n",
    "---\n",
    "Check that our backward and forward implementations return the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "b9RkH9-4eqw2"
   },
   "outputs": [],
   "source": [
    "emissions = crf.calculate_emissions(W_train)\n",
    "assert torch.all(\n",
    "    torch.isclose(\n",
    "        crf.backward_log_Z(W_train[:, 1:], emissions),\n",
    "        crf.forward_log_Z(W_train[:, 1:], emissions),\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BoqeybSRvDyC"
   },
   "source": [
    "## Q3c)\n",
    "---\n",
    "Check that our viterbi implementation and get_viterbi (from backpointer matrix) return the same\n",
    "results as the naive implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "HPHQf7ksetu8"
   },
   "outputs": [],
   "source": [
    "emissions = crf.calculate_emissions(W_train)\n",
    "\n",
    "for sentence in range(W_train.shape[0]):\n",
    "    for word_index in [2, 3, 4]:\n",
    "        score_naive, sequence_naive = crf.viterbi_naive(\n",
    "            W_train[sentence, :word_index],\n",
    "            emissions[\n",
    "                sentence,\n",
    "            ],\n",
    "        )\n",
    "        # print(sequence_naive)\n",
    "        score_viterbi, backpointers_viterbi = crf.backward_viterbi_log(\n",
    "            W_train[sentence, :word_index],\n",
    "            emissions[\n",
    "                sentence,\n",
    "            ],\n",
    "        )\n",
    "        sequence_viterbi = crf.get_viterbi(backpointers_viterbi)\n",
    "        # print(sequence_viterbi)\n",
    "        assert torch.isclose(score_viterbi[0, 0], score_naive)\n",
    "        assert torch.all(sequence_viterbi == sequence_naive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "-4hs8vTzyLZD"
   },
   "outputs": [],
   "source": [
    "# NB: THIS TEST IS NOT GRADED! It is just to help you double check \n",
    "# your implementation of Viterbi since the above test cases do not \n",
    "# contain instances of EOT/PAD.\n",
    "# NB2: Our evaluation expects Viterbi to only predict tags for actual \n",
    "# words, thus Viterbi (or get_viterbi) is supposed to remove instances\n",
    "# of EOS/BOS/PAD. Example: If Viterbi is asked to predict the sequence \n",
    "# [\"BOS\", \"I\", \"like\" \"dogs\", \"EOS\", \"PAD\", \"PAD\"], it should return a tagging \n",
    "# of length 3 (one for each valid word in the input).\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "crf = NeuralCRF(\n",
    "    pad_idx_word=pad_token_idx,\n",
    "    pad_idx_pos=pos_vocab[pad_token],\n",
    "    bos_idx=init_token_idx,\n",
    "    eos_idx=sep_token_idx,\n",
    "    bot_idx=pos_vocab[init_token],\n",
    "    eot_idx=pos_vocab[sep_token],\n",
    "    t_cal=T_CAL,\n",
    "    transformer=bert,\n",
    ")\n",
    "emissions = crf.calculate_emissions(W_train)\n",
    "sequences_overall = []\n",
    "first_batch_sequences_test = [torch.tensor([10, 13, 10,  4, 13, 10, 10,  0,  0, 10, 10,  0, 10,  5,  4,  1,  5,  6,\n",
    "         10,  4,  0, 10,  0, 10,  0, 10,  0, 19, 15]),\n",
    " torch.tensor([10, 10,  4,  8, 15, 14,  4, 14, 10,  4, 14, 10,  4, 14,  4,  4,  5,  5]),\n",
    " torch.tensor([10,  4, 13, 10,  4, 13, 10,  4, 13, 10,  4, 10,  4, 10,  4, 13, 10]),\n",
    " torch.tensor([10, 10,  5, 10,  4,  9,  4, 10,  4, 15, 10,  1,  6, 10,  6, 10]),\n",
    " torch.tensor([10, 10, 10, 13, 10,  4, 10,  6, 10, 13,  5, 10, 12, 10, 12, 14, 14,  6,\n",
    "          6,  6, 10, 10, 10,  6, 10,  6, 10, 10,  6, 10,  7,  6, 10,  6, 18, 10]),\n",
    " torch.tensor([10,  4, 19, 15,  4, 15,  4, 15, 10,  0, 10,  4,  5]),\n",
    " torch.tensor([ 5,  5, 10, 10, 10,  6,  8, 15, 15, 10,  5, 10,  5]),\n",
    " torch.tensor([10,  4, 15, 14, 10,  4,  4, 15,  8, 10,  9,  4,  5,  5,  5,  5]),\n",
    " torch.tensor([10,  6, 10,  0, 10,  0, 10,  0, 10,  4,  0, 15, 10, 10,  0, 15,  5,  6,\n",
    "         10,  4, 15, 14, 10, 10,  6, 10, 10, 10, 10,  6, 10,  0, 10,  0, 10]),\n",
    " torch.tensor([10, 10, 10, 12, 10,  4,  2, 15,  8, 15, 14, 10,  6, 10,  6, 10,  6, 10,\n",
    "          4,  5]),\n",
    " torch.tensor([10, 13, 10,  4, 13, 10, 10,  5, 10,  6, 10,  4,  5,  5, 10,  5,  5, 10,\n",
    "          5]),\n",
    " torch.tensor([14, 13,  4, 15,  8, 15,  8, 10,  4,  4,  5, 10,  4,  5, 10,  5, 10,  6,\n",
    "         10, 17,  5,  5, 10,  4,  4, 19, 15,  0, 10]),\n",
    " torch.tensor([10,  4,  4, 15, 10,  4,  5, 10,  4,  5, 10,  4,  5, 10,  4,  4,  5,  5,\n",
    "          5]),\n",
    " torch.tensor([10,  8, 15, 14,  4,  5, 10,  4, 15,  8, 10,  4,  4, 15, 15, 15,  8]),\n",
    " torch.tensor([14, 10, 10, 10,  4, 17,  5, 10, 15, 14, 10,  0, 14,  4, 14, 10,  4,  4,\n",
    "         13, 15, 15, 10,  6,  8, 15,  8, 15]),\n",
    " torch.tensor([10, 10,  4,  1, 14, 10, 10,  4, 10,  4, 10,  4, 19, 19, 14,  4, 10,  4,\n",
    "         10,  4,  4,  4,  5, 10,  4, 15]),\n",
    " torch.tensor([15, 13, 10,  6,  8, 10,  4,  2, 15,  1, 15, 15, 10,  1, 14,  4, 15, 14,\n",
    "          5, 10, 10, 10, 10,  1,  7,  0, 15]),\n",
    " torch.tensor([10,  4,  0, 10, 10,  0, 10,  4, 13, 10, 10]),\n",
    " torch.tensor([10, 10, 13, 15, 14, 10, 15, 14,  4, 10,  6, 10, 10,  4, 13,  4, 13, 10,\n",
    "          4,  5]),\n",
    " torch.tensor([10,  4, 14,  5,  5,  5,  5,  5,  5]),\n",
    " torch.tensor([10, 10,  7, 10,  8, 15, 14, 10, 15, 14, 10,  5,  5,  5, 10]),\n",
    " torch.tensor([17,  5, 10, 10,  6, 10,  5,  5, 10,  4,  5,  4,  4,  5,  5,  5]),\n",
    " torch.tensor([10,  4, 15, 14, 10,  5,  6, 15,  8, 14, 10,  5, 10,  5,  6, 10,  5]),\n",
    " torch.tensor([10,  4,  4, 15,  8, 15,  7, 10,  4, 15, 14,  4, 13, 10,  4,  5,  4, 10,\n",
    "          5,  6,  5,  5, 10,  5,  6, 15,  8, 15,  8, 10,  0, 10,  4, 19, 15,  6,\n",
    "         10,  6, 10,  5]),\n",
    " torch.tensor([10,  4, 19, 15, 15,  7, 14,  4, 14,  4, 14, 18, 10,  4,  1, 15, 10, 10,\n",
    "         10, 10, 10, 10,  6,  6, 10, 10,  6, 10,  4,  6, 10,  6, 10,  6, 10,  0,\n",
    "         10, 10,  6, 10, 10,  8, 10,  9, 14, 10,  9,  5,  4]),\n",
    " torch.tensor([14, 10,  0, 10,  4,  0, 10,  4,  4,  5,  4,  4,  4,  4,  8, 10, 10,  0,\n",
    "         10,  4,  6, 10, 10,  6, 10,  4, 10,  4,  9,  5, 10,  4, 10,  0, 10,  0,\n",
    "         10,  6, 10, 10, 10,  0,  0, 10,  0, 10,  0, 10,  7, 10,  4]),\n",
    " torch.tensor([10,  6,  8, 15,  8, 15,  0, 15, 18,  1,  8, 15, 14]),\n",
    " torch.tensor([10,  4, 14, 11,  8, 18, 14, 14, 14,  5]),\n",
    " torch.tensor([10, 12, 10,  4,  8, 15, 14,  5,  5]),\n",
    " torch.tensor([10,  4, 10,  0, 10,  0, 10, 10,  0, 15, 10, 10, 10,  4, 10,  4,  0, 10,\n",
    "          0, 10,  0, 10,  0, 10,  4,  9, 10,  6, 10,  5]),\n",
    " torch.tensor([10, 10,  5, 10, 10,  0, 10,  5, 10,  5, 10,  5, 10,  5, 10,  4,  8, 15,\n",
    "          8, 17]),\n",
    " torch.tensor([ 8,  8, 10, 10, 10,  6, 10,  2,  8, 15, 10,  6, 10,  0,  8, 15, 14, 10,\n",
    "         10, 10,  2, 10,  6, 10, 18, 10])]\n",
    "\n",
    "for sentence in range(W_train.shape[0]):\n",
    "    score_viterbi, backpointers_viterbi = crf.backward_viterbi_log(\n",
    "        W_train[sentence, :],\n",
    "        emissions[\n",
    "            sentence,\n",
    "        ],\n",
    "    )\n",
    "    sequence_viterbi = crf.get_viterbi(backpointers_viterbi)\n",
    "    sequences_overall += [sequence_viterbi]\n",
    "\n",
    "assert torch.all(torch.tensor([torch.all(first_batch_sequences_test[ix] == sequences_overall[ix]) for ix in range(len(sequences_overall))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pwaRWIiVvLNQ"
   },
   "source": [
    "## Q3d)\n",
    "---\n",
    "Check that our dijkstras implementation gives us the same as our backward viterbi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Yfch1WjMe1-u"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "emissions = crf.calculate_emissions(W_train)\n",
    "\n",
    "for sentence in range(W_train.shape[0]):\n",
    "    for word_index in [2, 3, 4]:\n",
    "        start_time_viterbi = time.perf_counter()\n",
    "        score_viterbi, backpointers_viterbi = crf.backward_viterbi_log(\n",
    "            W_train[sentence, 1:word_index], emissions[sentence, :word_index]\n",
    "        )\n",
    "        end_time_viterbi = time.perf_counter()\n",
    "        # print(f\"Score viterbi: {score_viterbi[0, 0]}\")\n",
    "        # print(f\"Viterbi for sentence {sentence} word_index {word_index} took: {round(end_time_viterbi-start_time_viterbi, 3)}s\")\n",
    "\n",
    "        start_time_dijkstra = time.perf_counter()\n",
    "        #sequence_viterbi = crf.get_viterbi(backpointers_viterbi)\n",
    "        score_dijkstra, sequence_dijsktra, log_Z = crf.dijkstra_viterbi_log(\n",
    "            W_train[sentence, 1:word_index], emissions[sentence, :word_index]\n",
    "        )\n",
    "        unnormalized_score = score_dijkstra + torch.sum(\n",
    "            (W_train[sentence, 1:word_index] != crf.eos_idx) * (W_train[sentence, 1:word_index] != crf.pad_idx_word)\n",
    "        ) * log_Z\n",
    "        end_time_dijkstra = time.perf_counter()\n",
    "        # print(f\"Unnormalized Score dijkstra: {unnormalized_score.item()}\")\n",
    "        # print(f\"Dijkstras for sentence {sentence} word_index {word_index} took: {round(end_time_dijkstra-start_time_dijkstra, 3)}s\")\n",
    "\n",
    "        assert torch.isclose(\n",
    "            score_viterbi[0, 0],\n",
    "                score_dijkstra\n",
    "                + torch.sum(\n",
    "                    (W_train[sentence, 1:word_index] != crf.eos_idx)\n",
    "                    * (W_train[sentence, 1:word_index] != crf.pad_idx_word)\n",
    "                ) * log_Z,\n",
    "        )\n",
    "        #assert torch.all(sequence_viterbi == sequence_dijsktra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RknzYtrUvln7"
   },
   "source": [
    "## Q3e)\n",
    "**My answer:** We see that Dijkstra's has a faster average runtime, but with much higher standard deviation. This is exactly what we described/expected and is indicative of the Non-greedy Vs. greedy algorithms we see in Viterbi Vs. Dijkstra's, stemming from the fact that with Dijkstra's we can be much faster when stopping early, but how early we stop can be inconsistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "49kT9v7yfO3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.72 ms  267 s per loop (mean  std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 10\n",
    "score_viterbi, backpointers_viterbi = crf.backward_viterbi_log(W_train[0, 1:4], emissions[0, :])\n",
    "#sequence_viterbi = crf.get_viterbi(backpointers_viterbi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "-cHJ0nUdfhVo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.91 s  174 ms per loop (mean  std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 10\n",
    "score_dijkstra, sequence_dijsktra, log_Z = crf.dijkstra_viterbi_log(W_train[0, 1:4], emissions[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "4P8rf_32vo-x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 s  124 ms per loop (mean  std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 10\n",
    "score_naive, sequence_naive = crf.viterbi_naive(W_train[0, 1:4], emissions[0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zz5Gely7wU9X"
   },
   "source": [
    "## Q3f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "ho8m3Vs-wVum"
   },
   "outputs": [],
   "source": [
    "def train_model_report_accuracy(\n",
    "    crf,\n",
    "    lr,\n",
    "    epochs,\n",
    "    train_dataloader,\n",
    "    dev_dataloader,\n",
    "    pad_token_idx_word,\n",
    "    pad_token_idx_tag,\n",
    "):\n",
    "\n",
    "    \"\"\"Train model for `epochs` epochs and report performance on \n",
    "        dev set after each epoch.\n",
    "\n",
    "    Parameters\n",
    "    ---\n",
    "    crf : NeuralCRF\n",
    "    lr : float\n",
    "        Learning rate to train with.\n",
    "    epochs : int\n",
    "        For how many epochs to train.\n",
    "    train_dataloader : torch.DataLoader\n",
    "    dev_dataloder : torch.DataLoader\n",
    "    pad_token_idx_word : int\n",
    "        Index with which to pad the word indices.\n",
    "    pad_token_idx_tag : int\n",
    "        Index with which to pad the tag indices.\n",
    "    \"\"\"\n",
    "    crf.train(True)\n",
    "    optimizer = torch.optim.Adam(crf.parameters(), lr=lr)\n",
    "    for epoch in range(epochs):\n",
    "        for i, data in enumerate(train_dataloader):\n",
    "            torch.autograd.set_detect_anomaly(True)\n",
    "            W = F.to_tensor(data[\"words\"], padding_value=pad_token_idx_word)\n",
    "            T = F.to_tensor(data[\"pos\"], padding_value=pad_token_idx_tag)\n",
    "            for param in crf.parameters():\n",
    "                param.grad = None\n",
    "            loss = crf.loss(T, W)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        with torch.no_grad():\n",
    "            predicted_sequences = []\n",
    "            true_sequences = []\n",
    "            for i_dev, data_dev in enumerate(valid_dataloader):\n",
    "                W_dev = F.to_tensor(\n",
    "                    data_dev[\"words\"], padding_value=pad_token_idx_word\n",
    "                )\n",
    "                T_dev = F.to_tensor(\n",
    "                    data_dev[\"pos\"], padding_value=pad_token_idx_tag\n",
    "                )\n",
    "                sequence_viterbi = crf(W_dev)\n",
    "                predicted_sequences += sequence_viterbi\n",
    "                for ix in range(W_dev.shape[0]):\n",
    "                    true_sequences += [\n",
    "                        T_dev[ix, 1 : (sequence_viterbi[ix].shape[0] + 1)]\n",
    "                    ]\n",
    "\n",
    "            acc = torch.tensor(0.0)\n",
    "            for ix in range(len(predicted_sequences)):\n",
    "                acc += torch.mean(\n",
    "                    (predicted_sequences[ix] == true_sequences[ix]).float()\n",
    "                )\n",
    "            acc = acc / len(predicted_sequences)\n",
    "            print(\"-------------------------\")\n",
    "            print(f\"Epoch: {epoch + 1} / {epochs}\")\n",
    "            print(f\"Development set accuracy: {acc}\")\n",
    "            print(\"-------------------------\")\n",
    "        epoch += 1\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "TYrbXAELyRQR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "Epoch: 1 / 3\n",
      "Development set accuracy: 0.8900293707847595\n",
      "-------------------------\n",
      "-------------------------\n",
      "Epoch: 2 / 3\n",
      "Development set accuracy: 0.903713583946228\n",
      "-------------------------\n",
      "-------------------------\n",
      "Epoch: 3 / 3\n",
      "Development set accuracy: 0.9065843224525452\n",
      "-------------------------\n",
      "This cell took: 9809.47s to run.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "crf = NeuralCRF(\n",
    "    pad_idx_word=pad_token_idx,\n",
    "    pad_idx_pos=pos_vocab[pad_token],\n",
    "    bos_idx=init_token_idx,\n",
    "    eos_idx=sep_token_idx,\n",
    "    bot_idx=pos_vocab[init_token],\n",
    "    eot_idx=pos_vocab[sep_token],\n",
    "    t_cal=T_CAL,\n",
    "    transformer=bert,\n",
    ")\n",
    "train_model_report_accuracy(\n",
    "    crf,\n",
    "    LR,\n",
    "    EPOCHS,\n",
    "    train_dataloader,\n",
    "    valid_dataloader,\n",
    "    pad_token_idx,\n",
    "    pos_vocab[pad_token],\n",
    ")\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "time_elapsed = end_time - start_time\n",
    "print(f\"This cell took: {round(time_elapsed, 2)}s to run.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7j3Xr0HBxhq8"
   },
   "source": [
    "## Q3g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "NgAws_iGxiX6"
   },
   "outputs": [],
   "source": [
    "emissions = crf.calculate_emissions(W_train)\n",
    "\n",
    "for sentence in range(W_train.shape[0]):\n",
    "    for word_index in [2, 3, 4]:\n",
    "        assert torch.isclose(\n",
    "            crf.entropy_naive(\n",
    "                W_train[sentence, :word_index],\n",
    "                emissions[\n",
    "                    sentence,\n",
    "                ],\n",
    "            ),\n",
    "            crf.backward_entropy(W_train[:, 1:word_index], emissions)[sentence],\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "jIKnm3b_Bm1N"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "Epoch: 1 / 3\n",
      "Development set accuracy: 0.9130592942237854\n",
      "-------------------------\n",
      "-------------------------\n",
      "Epoch: 2 / 3\n",
      "Development set accuracy: 0.9192327260971069\n",
      "-------------------------\n",
      "-------------------------\n",
      "Epoch: 3 / 3\n",
      "Development set accuracy: 0.9191904067993164\n",
      "-------------------------\n",
      "This cell took: 19025.75s to run.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.perf_counter()\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "entropy_regularized_crf = NeuralCRF(\n",
    "    pad_idx_word=pad_token_idx,\n",
    "    pad_idx_pos=pos_vocab[pad_token],\n",
    "    bos_idx=init_token_idx,\n",
    "    eos_idx=sep_token_idx,\n",
    "    bot_idx=pos_vocab[init_token],\n",
    "    eot_idx=pos_vocab[sep_token],\n",
    "    t_cal=T_CAL,\n",
    "    transformer=bert,\n",
    "    beta=10.0,\n",
    ")\n",
    "train_model_report_accuracy(\n",
    "    entropy_regularized_crf,\n",
    "    LR,\n",
    "    EPOCHS,\n",
    "    train_dataloader,\n",
    "    valid_dataloader,\n",
    "    pad_token_idx,\n",
    "    pos_vocab[pad_token],\n",
    ")\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "time_elapsed = end_time - start_time\n",
    "print(f\"This cell took: {round(time_elapsed, 2)}s to run.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "wdhhpDDfyj-8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "Epoch: 1 / 3\n",
      "Development set accuracy: 0.9213711023330688\n",
      "-------------------------\n",
      "-------------------------\n",
      "Epoch: 2 / 3\n",
      "Development set accuracy: 0.9213385581970215\n",
      "-------------------------\n",
      "-------------------------\n",
      "Epoch: 3 / 3\n",
      "Development set accuracy: 0.9206153750419617\n",
      "-------------------------\n",
      "This cell took: 18342.81s to run.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.perf_counter()\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "entropy_regularized_crf = NeuralCRF(\n",
    "    pad_idx_word=pad_token_idx,\n",
    "    pad_idx_pos=pos_vocab[pad_token],\n",
    "    bos_idx=init_token_idx,\n",
    "    eos_idx=sep_token_idx,\n",
    "    bot_idx=pos_vocab[init_token],\n",
    "    eot_idx=pos_vocab[sep_token],\n",
    "    t_cal=T_CAL,\n",
    "    transformer=bert,\n",
    "    beta=1.0,\n",
    ")\n",
    "train_model_report_accuracy(\n",
    "    entropy_regularized_crf,\n",
    "    LR,\n",
    "    EPOCHS,\n",
    "    train_dataloader,\n",
    "    valid_dataloader,\n",
    "    pad_token_idx,\n",
    "    pos_vocab[pad_token],\n",
    ")\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "time_elapsed = end_time - start_time\n",
    "print(f\"This cell took: {round(time_elapsed, 2)}s to run.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "DmCPsmTtym3U"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "Epoch: 1 / 3\n",
      "Development set accuracy: 0.9178801774978638\n",
      "-------------------------\n",
      "-------------------------\n",
      "Epoch: 2 / 3\n",
      "Development set accuracy: 0.9234488010406494\n",
      "-------------------------\n",
      "-------------------------\n",
      "Epoch: 3 / 3\n",
      "Development set accuracy: 0.9205983877182007\n",
      "-------------------------\n",
      "This cell took: 18493.23s to run.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.perf_counter()\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "entropy_regularized_crf = NeuralCRF(\n",
    "    pad_idx_word=pad_token_idx,\n",
    "    pad_idx_pos=pos_vocab[pad_token],\n",
    "    bos_idx=init_token_idx,\n",
    "    eos_idx=sep_token_idx,\n",
    "    bot_idx=pos_vocab[init_token],\n",
    "    eot_idx=pos_vocab[sep_token],\n",
    "    t_cal=T_CAL,\n",
    "    transformer=bert,\n",
    "    beta=0.1,\n",
    ")\n",
    "train_model_report_accuracy(\n",
    "    entropy_regularized_crf,\n",
    "    LR,\n",
    "    EPOCHS,\n",
    "    train_dataloader,\n",
    "    valid_dataloader,\n",
    "    pad_token_idx,\n",
    "    pos_vocab[pad_token],\n",
    ")\n",
    "end_time = time.perf_counter()\n",
    "time_elapsed = end_time - start_time\n",
    "print(f\"This cell took: {round(time_elapsed, 2)}s to run.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Nov  1 2022, 14:18:21) [GCC 12.2.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "f3c504e231ac2bd81064ef97d9b9fe7e2fbaa2e4945589ec3903de2c6c1aff32"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0083e082a6c7445dbdad6dc53066bd05": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_473bd59f2a77417481ffd230e64106dc",
      "placeholder": "",
      "style": "IPY_MODEL_fe52158d139c44bb9640d92b0cdd663d",
      "value": " 28.0/28.0 [00:00&lt;00:00, 655B/s]"
     }
    },
    "016cce9cee9343dab3d7253193094520": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "18bbe330643f4b198691ef13619fb7af": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1f9d911439044e46b9f89d0aea4ce41e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2341945bb058475d9854553f331a088b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "28ca8be1f7564e1ba9139de0b4faae88": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2c338c4a39fe41cb9765c549d84b85b9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_016cce9cee9343dab3d7253193094520",
      "max": 570,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ae4f9ae44cc64926b06864de8669d468",
      "value": 570
     }
    },
    "2c4e9e0c563f49c890e340e9b5f27f7b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2f91928b30444f48ba9b3996b637c651": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "373974851b6b40b58c157fbd4ee63eae": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3b5662f987654b3e97d41a8a88f3dec5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_18bbe330643f4b198691ef13619fb7af",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a6dda7b753b040d693cc141d3cd2f150",
      "value": 231508
     }
    },
    "43d6ebaa410045d0bf729345ea25172a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5b6f142f2844455cab53a7fda33f4cb6",
       "IPY_MODEL_cdeaffb2377f49bdbd0ef9c459542a5f",
       "IPY_MODEL_0083e082a6c7445dbdad6dc53066bd05"
      ],
      "layout": "IPY_MODEL_4b5b236ab1e34425826c6396b71257a3"
     }
    },
    "473bd59f2a77417481ffd230e64106dc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4b268bd0b12a4249a9dd0ff908538c7f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4b5b236ab1e34425826c6396b71257a3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "505cbe89e2384292aa5c64ecc080d610": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7d913c7b1f1b4986a7396bd0a692a0f0",
       "IPY_MODEL_2c338c4a39fe41cb9765c549d84b85b9",
       "IPY_MODEL_533d31dc5c604d9287ad695c6cb96edd"
      ],
      "layout": "IPY_MODEL_b94d16b7795e4e7485ea8fd564d9bc70"
     }
    },
    "51d29018f38c493a823622403fb69f6c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "533d31dc5c604d9287ad695c6cb96edd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f9cc3df591bc4122beeed2a3371cfa9f",
      "placeholder": "",
      "style": "IPY_MODEL_5729f1f235c44fdf9c04210a1a7cf6c6",
      "value": " 570/570 [00:00&lt;00:00, 10.4kB/s]"
     }
    },
    "5729f1f235c44fdf9c04210a1a7cf6c6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5b6f142f2844455cab53a7fda33f4cb6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1f9d911439044e46b9f89d0aea4ce41e",
      "placeholder": "",
      "style": "IPY_MODEL_a6606733bcdb4f6ea7f6d4a90ace57b7",
      "value": "Downloading: 100%"
     }
    },
    "5b71b3392fb04c3a864051627fe20d22": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5e655e51ceac40f2b622d3c729ef4076": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "620bfa45d32a4f54975c80619759be15": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c55a934128244302851e9f140786f497",
      "placeholder": "",
      "style": "IPY_MODEL_d2ed12c99aa74a37b11c5febfa35097f",
      "value": " 232k/232k [00:00&lt;00:00, 606kB/s]"
     }
    },
    "631673d7c05b4cbd9c39e77558ade025": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6fcafafc3d2e4bdbad2839393c573fad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2c4e9e0c563f49c890e340e9b5f27f7b",
      "placeholder": "",
      "style": "IPY_MODEL_5e655e51ceac40f2b622d3c729ef4076",
      "value": "Downloading: 100%"
     }
    },
    "74d55b0c6da54a8dbd5391ce1decbd3c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "77c97e95b2c2403180bf2416029709cc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7d913c7b1f1b4986a7396bd0a692a0f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eeda478a90e249978358336ca738c55a",
      "placeholder": "",
      "style": "IPY_MODEL_aac730ab374f4f119bdb124fc604d9f0",
      "value": "Downloading: 100%"
     }
    },
    "8471d000ef1945969fec179cbe2e88f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6fcafafc3d2e4bdbad2839393c573fad",
       "IPY_MODEL_aef9faf0373b4386817e0999df750166",
       "IPY_MODEL_9ecda6f850bb4bac82fe48923853c3ff"
      ],
      "layout": "IPY_MODEL_77c97e95b2c2403180bf2416029709cc"
     }
    },
    "8b8b98e85c414bfebd0afa36876447c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9969e49d1734463aa3a7fc133a41f8c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2f91928b30444f48ba9b3996b637c651",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_28ca8be1f7564e1ba9139de0b4faae88",
      "value": 0
     }
    },
    "9ecda6f850bb4bac82fe48923853c3ff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_51d29018f38c493a823622403fb69f6c",
      "placeholder": "",
      "style": "IPY_MODEL_2341945bb058475d9854553f331a088b",
      "value": " 440M/440M [00:14&lt;00:00, 47.6MB/s]"
     }
    },
    "a6606733bcdb4f6ea7f6d4a90ace57b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a6dda7b753b040d693cc141d3cd2f150": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a80ce5e04f6f4bd4a34f551c44574503": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c5cf1737a8e641fba46c4a01c69d1305",
       "IPY_MODEL_3b5662f987654b3e97d41a8a88f3dec5",
       "IPY_MODEL_620bfa45d32a4f54975c80619759be15"
      ],
      "layout": "IPY_MODEL_c94dbb98558a4dd296eb8de93d425176"
     }
    },
    "a95fbd63e826417ebf8af09444e676d0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "aac730ab374f4f119bdb124fc604d9f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ae4f9ae44cc64926b06864de8669d468": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "aef9faf0373b4386817e0999df750166": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c4b4b9e02c184a38824e57dee94ae7d3",
      "max": 440473133,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8b8b98e85c414bfebd0afa36876447c4",
      "value": 440473133
     }
    },
    "b53f65341be24333939cd373696bd403": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_de8774404f7c4879950013b96feadc0a",
       "IPY_MODEL_9969e49d1734463aa3a7fc133a41f8c1",
       "IPY_MODEL_ca22e587adde43f09d124961d24e9586"
      ],
      "layout": "IPY_MODEL_c4e2088b7959485fb58b4db74d924ab2"
     }
    },
    "b94d16b7795e4e7485ea8fd564d9bc70": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c4b4b9e02c184a38824e57dee94ae7d3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c4e2088b7959485fb58b4db74d924ab2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c55a934128244302851e9f140786f497": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c5cf1737a8e641fba46c4a01c69d1305": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5b71b3392fb04c3a864051627fe20d22",
      "placeholder": "",
      "style": "IPY_MODEL_4b268bd0b12a4249a9dd0ff908538c7f",
      "value": "Downloading: 100%"
     }
    },
    "c94dbb98558a4dd296eb8de93d425176": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ca22e587adde43f09d124961d24e9586": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ea3b17a58fac4d9f974d9200f29983f6",
      "placeholder": "",
      "style": "IPY_MODEL_d540aa212f6a4335a375b242cbafe235",
      "value": " 0/0 [00:00&lt;?, ?it/s]"
     }
    },
    "cdeaffb2377f49bdbd0ef9c459542a5f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_631673d7c05b4cbd9c39e77558ade025",
      "max": 28,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a95fbd63e826417ebf8af09444e676d0",
      "value": 28
     }
    },
    "d2ed12c99aa74a37b11c5febfa35097f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d540aa212f6a4335a375b242cbafe235": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "de8774404f7c4879950013b96feadc0a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_373974851b6b40b58c157fbd4ee63eae",
      "placeholder": "",
      "style": "IPY_MODEL_74d55b0c6da54a8dbd5391ce1decbd3c",
      "value": ""
     }
    },
    "ea3b17a58fac4d9f974d9200f29983f6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eeda478a90e249978358336ca738c55a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f9cc3df591bc4122beeed2a3371cfa9f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fe52158d139c44bb9640d92b0cdd663d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
